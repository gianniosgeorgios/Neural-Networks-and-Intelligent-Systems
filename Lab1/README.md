# Supervised Learning in UCI Machine Learning Dataset 

## Dataset 1: Japanese Credit Screening

### Data Information: 
Number of Classes: 2 <br/>
Examples represent positive and negative instances of people who were and were not granted credit. 
The theory was generated by talking to the individuals at a Japanese company that grants credit.

### Preproccesing: 

* **Missing values** : We don't ignore columns with missing values. Instead, in numeric data we apply "mean strategy" and in string data, "most frequent strategy". 
* **Non Ordinal features**: We convert categorical non ordinal features to binary vectors

### Classification:

We compare two scikit-learn classifiers: kNN and Gaussian Naive Bayes (GNB).

### Optimization:

Fine tuning using PipeLine, GridSearch and Cross Validation techniques. The steps of pipeline were: variance threshold selector, standard scaler, random over sampler and PCA. 

### Results:

The accuracy (after kNN Optimization) we achieved was 92%. 

## Dataset 2: Statlog (Landsat Satellite) Data Set

### Data Information:

Number of Classes: 7 <br/>
<br/>
The database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. The aim is to predict this classification, given the multi-spectral values.
The Landsat satellite data is one of the many sources of information available for a scene. The interpretation of a scene by integrating spatial data of diverse types and resolutions including multispectral and radar data, maps indicating topography, land use etc. is expected to assume significant importance with the onset of an era characterised by integrative approaches to remote sensing (for example, NASA's Earth Observing System commencing this decade).Classes are: <br/>
1. red soil
2. cotton crop
3. grey soil
4. damp grey soil
5. soil with vegetation stubble
6. mixture class (all types present)
7. very damp grey soil

### Preproccesing: 

Since all features are numerical, and the dataset has not missing values no preproccesing needed.

### Classification:

We compare different scikit-learn classifiers: Gaussian Naive Bayes, kNN, Multi-Layer Perceptron (MLP), Support Vector Machines (SVM)

### Optimization:

Fine tuning using PipeLine, GridSearch and Cross Validation techniques. The steps of pipeline were: variance threshold selector, standard scaler, random over sampler and PCA.

### Results:

The accuracy we achieved was 90% using SVM with polynomial kernel.
