# -*- coding: utf-8 -*-
"""Lab3_Team70.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BQZreZ7lI7HXjvq9i_a8mPH5-ettGXZM

# Βαθιά μάθηση στο CIFAR-100

**Ομάδα**: $\;\;\;$70 <br> **Φοιτητές**: Γιαννιός Γεώργιος Ταξιάρχης 031 16 156$\;\;\;\;\;\;\;\;\;\;\;\;\;$ Μπέτζελος Γιώργος 031 17 442  $\;\;\;\;\;\;\;\;\;\;\;\;\;$ Μπέτζελος Χρήστος 031 16 067 <br>

## Εισαγωγή και επισκόπηση του συνόλου δεδομένων
"""

from __future__ import absolute_import, division, print_function, unicode_literals # legacy compatibility

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Συνάρτηση για υπολογισμό memory usage με βαση τις παραμέτρους (για μια εικόνα)

#Πήγη 
#https://stackoverflow.com/questions/43137288/how-to-determine-needed-memory-of-keras-model
def get_model_memory_usage(batch_size, model):
    import numpy as np
    try:
        from keras import backend as K
    except:
        from tensorflow.keras import backend as K

    shapes_mem_count = 0
    internal_model_mem_count = 0
    for l in model.layers:
        layer_type = l.__class__.__name__
        if layer_type == 'Model':
            internal_model_mem_count += get_model_memory_usage(batch_size, l)
        single_layer_mem = 1
        out_shape = l.output_shape
        if type(out_shape) is list:
            out_shape = out_shape[0]
        for s in out_shape:
            if s is None:
                continue
            single_layer_mem *= s
        shapes_mem_count += single_layer_mem

    trainable_count = np.sum([K.count_params(p) for p in model.trainable_weights])
    non_trainable_count = np.sum([K.count_params(p) for p in model.non_trainable_weights])

    number_size = 4.0
    if K.floatx() == 'float16':
        number_size = 2.0
    if K.floatx() == 'float64':
        number_size = 8.0

    total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)
    gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count
    return gbytes

# helper functions

# select from from_list elements with index in index_list
def select_from_list(from_list, index_list):
  filtered_list= [from_list[i] for i in index_list]
  return(filtered_list)

# append in filtered_list the index of each element of unfilterd_list if it exists in in target_list
def get_ds_index(unfiliterd_list, target_list):
  index = 0
  filtered_list=[]
  for i_ in unfiliterd_list:
    if i_[0] in target_list:
      filtered_list.append(index)
    index += 1
  return(filtered_list)

# select a url for a unique subset of CIFAR-100 with 20, 40, 60, or 80 classes
def select_classes_number(classes_number = 20):
  cifar100_20_classes_url = "https://pastebin.com/raw/nzE1n98V"
  cifar100_40_classes_url = "https://pastebin.com/raw/zGX4mCNP"
  cifar100_60_classes_url = "https://pastebin.com/raw/nsDTd3Qn"
  cifar100_80_classes_url = "https://pastebin.com/raw/SNbXz700"
  if classes_number == 20:
    return cifar100_20_classes_url
  elif classes_number == 40:
    return cifar100_40_classes_url
  elif classes_number == 60:
    return cifar100_60_classes_url
  elif classes_number == 80:
    return cifar100_80_classes_url
  else:
    return -1

# load the entire dataset
(x_train_all, y_train_all), (x_test_all, y_test_all) = tf.keras.datasets.cifar100.load_data(label_mode='fine')

print(x_train_all.shape)

"""Η κάθε ομάδα θα δουλέψει με ένα μοναδικό ξεχωριστό υποσύνολο του CIFAR-100
Στο επόμενο κελί, αντικαταστήστε την τιμή της μεταβλητής `team_seed` με τον αριθμό της ομάδας σας.
"""

# REPLACE WITH YOUR TEAM NUMBER
team_seed = 70

"""Στο επόμενο κελί μπορείτε να διαλέξετε το πλήθος των κατηγορίων σας: 20 (default), 40, 60 ή 80."""

# select the number of classes
cifar100_classes_url = select_classes_number(classes_number = 40)

"""Δημιουργούμε το μοναδικό dataset της ομάδας μας:"""

team_classes = pd.read_csv(cifar100_classes_url, sep=',', header=None)
CIFAR100_LABELS_LIST = pd.read_csv('https://pastebin.com/raw/qgDaNggt', sep=',', header=None).astype(str).values.tolist()[0]

our_index = team_classes.iloc[team_seed,:].values.tolist()
our_classes = select_from_list(CIFAR100_LABELS_LIST, our_index)
train_index = get_ds_index(y_train_all, our_index)
test_index = get_ds_index(y_test_all, our_index)

x_train_ds = np.asarray(select_from_list(x_train_all, train_index))
y_train_ds = np.asarray(select_from_list(y_train_all, train_index))
x_test_ds = np.asarray(select_from_list(x_test_all, test_index))
y_test_ds = np.asarray(select_from_list(y_test_all, test_index))

# print our classes
print(our_classes)

CLASSES_NUM=len(our_classes)

print(x_train_ds[1].shape)

# get (train) dataset dimensions
data_size, img_rows, img_cols, img_channels = x_train_ds.shape

# set validation set percentage (wrt the training set size)
validation_percentage = 0.15
val_size = round(validation_percentage * data_size)

# Reserve val_size samples for validation and normalize all values
x_val = x_train_ds[-val_size:]/255
y_val = y_train_ds[-val_size:]
x_train = x_train_ds[:-val_size]/255
y_train = y_train_ds[:-val_size]
x_test = x_test_ds/255
y_test = y_test_ds

print(len(x_val))

# summarize loaded dataset
print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))
print('Validation: X=%s, y=%s' % (x_val.shape, y_val.shape))
print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))

# get class label from class index
def class_label_from_index(fine_category):
  return(CIFAR100_LABELS_LIST[fine_category.item(0)])

# plot first few images
plt.figure(figsize=(6, 6))
for i in range(9):
	# define subplot
  plt.subplot(330 + 1 + i).set_title(class_label_from_index(y_train[i]))
	# plot raw pixel data
  plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))
  #show the figure
plt.show()

"""## Συναρτήσεις εκπαίδευσης

Θα χρησιμοποιήσουμε την ιδιότητα data prefetch του tf2:
"""

# we user prefetch https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch 
# see also AUTOTUNE
# the dataset is now "infinite"

BATCH_SIZE = 128
AUTOTUNE = tf.data.experimental.AUTOTUNE # https://www.tensorflow.org/guide/data_performance

def _input_fn(x,y, BATCH_SIZE):
  ds = tf.data.Dataset.from_tensor_slices((x,y))
  ds = ds.shuffle(buffer_size=data_size)
  ds = ds.repeat()
  ds = ds.batch(BATCH_SIZE)
  ds = ds.prefetch(buffer_size=AUTOTUNE)
  return ds

def _input_fn_ds(ds, BATCH_SIZE):
  ds = ds.shuffle(buffer_size=data_size)
  ds = ds.repeat()
  ds = ds.batch(BATCH_SIZE)
  ds = ds.prefetch(buffer_size=AUTOTUNE)
  return ds


# steps_per_epoch and validation_steps for training and validation: https://www.tensorflow.org/guide/keras/train_and_evaluate

def train_model(model, epochs = 10, steps_per_epoch = 2, validation_steps = 1, image_size=32,callback = None):
  
  if image_size == 32:
    train_ds = train_ds_32
    validation_ds = validation_ds_32

  elif image_size == 78:
    train_ds = train_ds_78
    validation_ds = validation_ds_78
  
  elif image_size == 96:
    train_ds = train_ds_96
    validation_ds = validation_ds_96

  elif image_size == 128:
    train_ds = train_ds_128
    validation_ds = validation_ds_128
  
  if (callback == None):
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  else:
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps,callbacks=[callback])

  return (history)

"""## Γραφικές παραστάσεις εκπαίδευσης και απόδοση στο σύνολο ελέγχου"""

# plot diagnostic learning curves
def summarize_diagnostics(history):
	plt.figure(figsize=(8, 8))
	plt.suptitle('Training Curves')
	# plot loss
	plt.subplot(211)
	plt.title('Cross Entropy Loss')
	plt.plot(history.history['loss'], color='blue', label='train')
	plt.plot(history.history['val_loss'], color='orange', label='val')
	plt.legend(loc='upper right')
	# plot accuracy
	plt.subplot(212)
	plt.title('Classification Accuracy')
	plt.plot(history.history['accuracy'], color='blue', label='train')
	plt.plot(history.history['val_accuracy'], color='orange', label='val')
	plt.legend(loc='lower right')
	return plt
 
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps, image_size):
  if image_size==32:
    test_ds = test_ds_32
  elif image_size==78:
    test_ds = test_ds_78
  elif image_size==96:
    test_ds = test_ds_96
  elif image_size==128:
    test_ds = test_ds_128
  
  print('\nTest set evaluation metrics')
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("loss: {:.2f}".format(loss0))
  print("accuracy: {:.2f}".format(accuracy0))


def model_report(model, history, evaluation_steps = 10, image_size=32):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps, image_size)

"""## Διαχείριση μνήμης (TFRecord)"""

def _bytes_feature(value):
  """Returns a bytes_list from a string / byte."""
  if isinstance(value, type(tf.constant(0))):
    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _int64_feature(value):
  """Returns an int64_list from a bool / enum / int / uint."""
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def serialize_example(image, label, shape):
  """
  Creates a tf.train.Example message ready to be written to a file.
  """
  # Create a dictionary mapping the feature name to the tf.train.Example-compatible
  # data type.
  feature = {
      'image': _bytes_feature(image),
      'label': _int64_feature(label),
      'height': _int64_feature(shape[0]),
      'width': _int64_feature(shape[1]),
      'depth': _int64_feature(shape[2]),
  }

  # Create a Features message using tf.train.Example.

  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
  return example_proto.SerializeToString()

def write_tf_records(tfrecord_dir, x, y):
  with tf.io.TFRecordWriter(tfrecord_dir) as writer:
    for image, label in zip(x, y):
                
      image = tf.image.resize(image, (TARGET_SIZE[0], TARGET_SIZE[1]))
          
      img_bytes = tf.io.serialize_tensor(image)
      shape = image.shape
          
      example = serialize_example(img_bytes, label, shape)
      writer.write(example)
  return None

# Create a dictionary describing the features.
image_feature_description = {
    'image': tf.io.FixedLenFeature([], tf.string),
    'label': tf.io.FixedLenFeature([], tf.int64),
    'height': tf.io.FixedLenFeature([], tf.int64),
    'width': tf.io.FixedLenFeature([], tf.int64),
    'depth': tf.io.FixedLenFeature([], tf.int64),  
}

def _parse_image_function(example_proto):
  # Parse the input tf.train.Example proto using the dictionary above.
  example = tf.io.parse_single_example(example_proto, image_feature_description)
  label = example["label"]
  image_shape = TARGET_SIZE

  image = tf.io.parse_tensor(example["image"], float)
  image = tf.reshape(image, image_shape)

  return  image , [label]

def read_dataset(file):
  dataset = tf.data.TFRecordDataset(file)
  return dataset.map(_parse_image_function)

"""### Μέγεθος εικόνας (32x32)"""

train_ds_32 =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds_32 =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds_32 =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object

"""### Μέγεθος εικόνας (78x78)"""

TARGET_SIZE = (78,78,3)

if not os.path.exists('./tf_records_78'):
  os.makedirs('./tf_records_78')

write_tf_records("./tf_records_78/" + "train.tfrecords", x_train, y_train)
write_tf_records("./tf_records_78/" + "validation.tfrecords", x_val, y_val)
write_tf_records("./tf_records_78/" + "test.tfrecords", x_test, y_test)

train_ds_78 = read_dataset("./tf_records_78/" + "train.tfrecords")
validation_ds_78 = read_dataset("./tf_records_78/" + "validation.tfrecords")
test_ds_78 = read_dataset("./tf_records_78/" + "test.tfrecords")

train_ds_78 = _input_fn_ds(train_ds_78, BATCH_SIZE)
validation_ds_78 = _input_fn_ds(validation_ds_78, BATCH_SIZE)
test_ds_78 = _input_fn_ds(test_ds_78, BATCH_SIZE)

"""### Μέγεθος εικόνας (96x96)"""

TARGET_SIZE = (96,96,3)

if not os.path.exists('./tf_records_96'):
  os.makedirs('./tf_records_96')

write_tf_records("./tf_records_96/" + "train.tfrecords", x_train, y_train)
write_tf_records("./tf_records_96/" + "validation.tfrecords", x_val, y_val)
write_tf_records("./tf_records_96/" + "test.tfrecords", x_test, y_test)

train_ds_96 = read_dataset("./tf_records_96/" + "train.tfrecords")
validation_ds_96 = read_dataset("./tf_records_96/" + "validation.tfrecords")
test_ds_96 = read_dataset("./tf_records_96/" + "test.tfrecords")

train_ds_96 = _input_fn_ds(train_ds_96, BATCH_SIZE)
validation_ds_96 = _input_fn_ds(validation_ds_96, BATCH_SIZE)
test_ds_96 = _input_fn_ds(test_ds_96, BATCH_SIZE)

"""### Μέγεθος εικόνας (128x128)"""

TARGET_SIZE = (128,128,3)

if not os.path.exists('./tf_records_128'):
  os.makedirs('./tf_records_128')

write_tf_records("./tf_records_128/" + "train.tfrecords", x_train, y_train)
write_tf_records("./tf_records_128/" + "validation.tfrecords", x_val, y_val)
write_tf_records("./tf_records_128/" + "test.tfrecords", x_test, y_test)

train_ds_128 = read_dataset("./tf_records_128/" + "train.tfrecords")
validation_ds_128 = read_dataset("./tf_records_128/" + "validation.tfrecords")
test_ds_128 = read_dataset("./tf_records_128/" + "test.tfrecords")

train_ds_128 = _input_fn_ds(train_ds_128, BATCH_SIZE)
validation_ds_128 = _input_fn_ds(validation_ds_128, BATCH_SIZE)
test_ds_128 = _input_fn_ds(test_ds_128, BATCH_SIZE)

"""## Δοκιμές διαφορετικών μοντέλων

Δοκιμάζουμε μοντέλα "**from scratch**", όπου ορίζουμε την αρχιτεκτονική του δικτύου όπως θέλουμε, και επίσης δοκιμάζουμε μοντέλα χρησιμοποιώντας **μεταφορά μάθησης**.

### Μοντέλα "from scratch"

#### Μοντέλο 1: CNN (Μικρό Συνελικτικό Δίκτυο)
"""

# a simple CNN https://www.tensorflow.org/tutorials/images/cnn

def init_simple_model(summary):
  model = models.Sequential()
  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32,32,3)))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(layers.Flatten())
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(100, activation='softmax'))
  
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

SIMPLE_MODEL = init_simple_model(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, SIMPLE_MODEL)*1024,"MB")
print("")

start_time = time.time()
SIMPLE_MODEL_history = train_model(SIMPLE_MODEL, 200, 30, 5)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(SIMPLE_MODEL, SIMPLE_MODEL_history, 30)

"""#### Μοντέλο 2: CNN Custom"""

def init_model_2(summary):
  model_2 = models.Sequential()

  model_2.add(layers.Conv2D(32, (6, 6), activation='relu', input_shape=(32, 32, 3),padding = 'same'))
  model_2.add(layers.AveragePooling2D((2, 2),padding = 'same'))

  model_2.add(layers.Conv2D(16, (3, 3), activation='relu',padding = 'same'))
  model_2.add(layers.AveragePooling2D((2, 2),padding = 'same'))

  model_2.add(layers.Conv2D(32, (2, 2), activation='relu',padding = 'same'))
  model_2.add(layers.Flatten())

  model_2.add(layers.Dense(1024, activation='relu'))
  model_2.add(layers.Dense(200, activation='relu'))
  model_2.add(layers.Dense(100, activation='softmax'))

  model_2.compile(optimizer=tf.optimizers.Adamax(learning_rate=0.0001), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model_2.summary()
  return model_2

MODEL_2 = init_model_2(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, MODEL_2)*1024,"MB")
print("")

start_time = time.time()
MODEL_2_history = train_model(MODEL_2, 200, 30, 10)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")
model_report(MODEL_2, MODEL_2_history, 30)

"""#### Μοντέλο 3: LeNet"""

def init_lenet(summary):
  lenet = models.Sequential()
  lenet.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
  lenet.add(layers.MaxPooling2D((2, 2)))
  lenet.add(layers.Conv2D(32, (3, 3), activation='relu'))
  lenet.add(layers.MaxPooling2D((2, 2)))

  lenet.add(layers.Flatten())

  lenet.add(layers.Dense(512, activation='relu'))
  lenet.add(layers.Dense(200, activation='relu'))
  lenet.add(layers.Dense(100, activation='softmax'))

  lenet.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    lenet.summary()
  return lenet

LENET = init_lenet(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, LENET)*1024,"MB")
print("")

start_time = time.time()
LENET_history = train_model(LENET, 200, 30, 5)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(LENET, LENET_history, 30)

"""### Μεταφορά μάθησης

Χρησιμοποιούμε τη [μεταφορά μάθησης του tf2](https://www.tensorflow.org/tutorials/images/transfer_learning). Σε αντίθεση με τα μοντέλα "from scratch" η μεταφορά μάθησης μας επιστρέφει έτοιμα μοντέλα με προκαθορισμένη αρχιτεκτονική στην οποία μπορούμε γενικά μόνο να προσθέσουμε επίπεδα, τα οποία περιορίζονται σε πλήρως διασυνδεδεμένα επίπεδα που εξειδικεύονται στο συγκεκριμένο task ταξινόμησης που έχουμε να επιτελέσουμε.

#### Μοντέλο 1: VGG16
"""

def init_VGG16_model(summary):
  vgg_model=tf.keras.applications.VGG16(input_shape=(78,78,3), include_top=False, weights='imagenet')
  
  VGG16_MODEL=vgg_model.layers[0](vgg_model)

  # unfreeze conv layers
  VGG16_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([VGG16_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

VGG16_MODEL = init_VGG16_model(True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, VGG16_MODEL)*1024,"MB")
print("")

start_time = time.time()
VGG16_MODEL_history = train_model(VGG16_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(VGG16_MODEL, VGG16_MODEL_history, 30, 78)

"""#### Μοντέλο 2: Xception"""

def init_XCEPTION_model(summary):
  xception_model=tf.keras.applications.Xception(input_shape=(78,78,3), include_top=False, weights='imagenet')
  
  XCEPTION_MODEL=xception_model.layers[0](xception_model)

  # unfreeze conv layers
  XCEPTION_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  batch_layer = tf.keras.layers.BatchNormalization()
  dense_layer = tf.keras.layers.Dense(256, activation='relu')

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([XCEPTION_MODEL, dropout_layer, batch_layer, global_average_layer, dense_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

XCEPTION_MODEL = init_XCEPTION_model(True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, XCEPTION_MODEL)*1024,"MB")
print("")

start_time = time.time()
XCEPTION_MODEL_history = train_model(XCEPTION_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(XCEPTION_MODEL, XCEPTION_MODEL_history, 30, 78)

"""#### Μοντέλο 3: EfficientNetB7"""

def init_EFFICIENT_B7_model(summary):
  efficient_b7_model=tf.keras.applications.EfficientNetB7(input_shape=(78,78,3), include_top=False, weights='imagenet')
  
  EFFICIENT_B7_MODEL=efficient_b7_model.layers[0](efficient_b7_model)

  # unfreeze conv layers
  EFFICIENT_B7_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([EFFICIENT_B7_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

EFFICIENT_B7_MODEL = init_EFFICIENT_B7_model(True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, EFFICIENT_B7_MODEL)*1024,"MB")
print("")

start_time = time.time()
EFFICIENT_B7_MODEL_history = train_model(EFFICIENT_B7_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(EFFICIENT_B7_MODEL, EFFICIENT_B7_MODEL_history, 30, 78)

"""#### Μοντέλο 4: ResNet50"""

def init_RESNET50_model(summary):
  resnet50_model=tf.keras.applications.ResNet50(input_shape=(78,78,3), include_top=False, weights='imagenet')
  
  RESNET50_MODEL=resnet50_model.layers[0](resnet50_model)

  # unfreeze conv layers
  RESNET50_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([RESNET50_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

RESNET50_MODEL = init_RESNET50_model(True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, RESNET50_MODEL)*1024,"MB")
print("")

start_time = time.time()
RESNET50_MODEL_history = train_model(RESNET50_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(RESNET50_MODEL, RESNET50_MODEL_history, 30, 78)

"""#### Μοντέλο 5: ResNet152V2"""

def init_RESNET152_model(summary):
  resnet152_model=tf.keras.applications.ResNet152V2(input_shape=(78,78,3), include_top=False, weights='imagenet')
  
  RESNET152_MODEL=resnet152_model.layers[0](resnet152_model)

  # unfreeze conv layers
  RESNET152_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([RESNET152_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

RESNET152_MODEL = init_RESNET152_model(True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, RESNET152_MODEL)*1024,"MB")
print("")

start_time = time.time()
RESNET152_MODEL_history = train_model(RESNET152_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(RESNET152_MODEL, RESNET152_MODEL_history, 30, 78)

"""#### Μοντέλο 6: DenseNet121"""

def init_DENSE121_model(summary):
  dense121_model=tf.keras.applications.DenseNet121(input_shape=(78,78,3), include_top=False, weights='imagenet')
  
  DENSE121_MODEL=dense121_model.layers[0](dense121_model)

  # unfreeze conv layers
  DENSE121_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([DENSE121_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

DENSE121_MODEL = init_DENSE121_model(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE121_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE121_MODEL_history = train_model(DENSE121_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE121_MODEL, DENSE121_MODEL_history, 30,78)

"""#### Μοντέλο 7: DenseNet169"""

def init_DENSE169_model(summary, inp_size=78):
  dense169_model=tf.keras.applications.DenseNet169(input_shape=(inp_size,inp_size,3), include_top=False, weights='imagenet')
  
  DENSE169_MODEL=dense169_model.layers[0](dense169_model)

  # unfreeze conv layers
  DENSE169_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([DENSE169_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

DENSE169_MODEL = init_DENSE169_model(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE169_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE169_MODEL_history = train_model(DENSE169_MODEL, 50, 40, 10,78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE169_MODEL, DENSE169_MODEL_history, 30,78)

"""#### Μοντέλο 8: DenseNet201"""

def init_DENSE201_model(summary, inp_size=78, data_augment=False, train="All", dropout=0.5, learning_rate=0.00005, optimizer="Adam"):
  dense201_model=tf.keras.applications.DenseNet201(input_shape=(inp_size,inp_size,3), include_top=False, weights='imagenet')
  
  DENSE201_MODEL=dense201_model.layers[0](dense201_model)

  # Unfreeze conv layers (Συνολικά 707 layers)
  if train=="All":
    DENSE201_MODEL.trainable=True
  elif train=="10%":
    # εκαπαιδεύουμε τα τελευταία επίπεδα 71 (10%) 
    for layer in dense201_model.layers[:636]:
      layer.trainable = False
  elif train=="20%":
    # εκαπαιδεύουμε τα τελευταία επίπεδα 142 (20%) 
    for layer in dense201_model.layers[:565]:
      layer.trainable = False
  elif train=="None":
    # Freeze the pretrained weights
    dense201_model.trainable = False
  
  
  dropout_layer = tf.keras.layers.Dropout(rate = dropout)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  
  # Data augmentation
  if data_augment:
    model = tf.keras.Sequential([data_augmentation, DENSE201_MODEL, dropout_layer, global_average_layer, prediction_layer])
  else:
    model = tf.keras.Sequential([DENSE201_MODEL, dropout_layer, global_average_layer, prediction_layer])
  
  # Optimizer
  if optimizer=="Adam":
    model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  elif optimizer=="Adamax":
    model.compile(optimizer=tf.optimizers.Adamax(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  elif optimizer=="SGD":
    model.compile(optimizer=tf.optimizers.SGD(learning_rate=0.0005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  
  if summary: 
    model.summary()
  return model

DENSE201_MODEL = init_DENSE201_model(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 78)

"""#### Μοντέλο 9: ΜobileNetV2"""

# transfer learning: Input size must be at least 75x75;

def init_MOBILE_NET_model(summary):
  mobile_net_model=tf.keras.applications.MobileNetV2(input_shape=(78,78,3), include_top=False, weights='imagenet')
  
  MOBILE_NET_MODEL=mobile_net_model.layers[0](mobile_net_model)

  # unfreeze conv layers
  MOBILE_NET_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([MOBILE_NET_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

MOBILE_NET_MODEL = init_MOBILE_NET_model(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, MOBILE_NET_MODEL)*1024,"MB")
print("")

start_time = time.time()
MOBILE_NET_MODEL_history = train_model(MOBILE_NET_MODEL, 50, 40, 10,78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(MOBILE_NET_MODEL, MOBILE_NET_MODEL_history, 30,78)

"""#### Μοντέλο 10: InceptionV3"""

def init_INCEPTION_V3_model(summary):
  inception_v3_model=tf.keras.applications.InceptionV3(input_shape=(78,78,3), include_top=False, weights='imagenet')
  
  INCEPTION_V3_MODEL=inception_v3_model.layers[0](inception_v3_model)

  # unfreeze conv layers
  INCEPTION_V3_MODEL.trainable=True
  
  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([INCEPTION_V3_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

INCEPTION_V3_MODEL = init_INCEPTION_V3_model(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, INCEPTION_V3_MODEL)*1024,"MB")
print("")

start_time = time.time()
INCEPTION_V3_MODEL_history = train_model(INCEPTION_V3_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(INCEPTION_V3_MODEL, INCEPTION_V3_MODEL_history, 30, 78)

"""## Βελτίωση της επίδοσης με πειράματα

Βελτιώνουμε τα αποτελέσματα ταξινόμησης στο CIFAR-100 και βγάζουμε συμπεράσματα, σύμφωνα με όσα ζητούνται σε σχέση με την αναφορά παράδοσης.

### Μέγεθος Εικόνας

#### Μέγεθος εικόνας (32x32)

##### DenseNet169
"""

DENSE169_MODEL = init_DENSE169_model(summary = True, inp_size=32)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE169_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE169_MODEL_history = train_model(DENSE169_MODEL, 50, 40, 10,32)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE169_MODEL, DENSE169_MODEL_history, 30,32)

"""##### DenseNet201"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=32)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30)

"""#### Μέγεθος εικόνας (78x78)

##### DenseNet169
"""

DENSE169_MODEL = init_DENSE169_model(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE169_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE169_MODEL_history = train_model(DENSE169_MODEL, 50, 40, 10,78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE169_MODEL, DENSE169_MODEL_history, 30,78)

"""##### DenseNet201"""

DENSE201_MODEL = init_DENSE201_model(summary = True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 78)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 78)

"""#### Μέγεθος εικόνας (96x96)

##### DenseNet169
"""

DENSE169_MODEL = init_DENSE169_model(summary = True, inp_size=96)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE169_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE169_MODEL_history = train_model(DENSE169_MODEL, 50, 40, 10,96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE169_MODEL, DENSE169_MODEL_history, 30,96)

"""##### DenseNet201"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### Data augmentation"""

data_augmentation = tf.keras.Sequential(
  [
    layers.experimental.preprocessing.RandomFlip("horizontal", input_shape=(96, 96,3)),
    layers.experimental.preprocessing.RandomRotation(0.1),
    layers.experimental.preprocessing.RandomZoom(0.1),
    layers.experimental.preprocessing.RandomContrast(0.1)
  ]
)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### Πλήθος trainable επιπέδων

#### Freeze All
"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, train="None")
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Unfreeze 10 %"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, train="10%")

print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Unfreeze 20 %"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, train="20%")
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Unfreeze All"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### Dropout

#### Dropout = 0.15
"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, dropout=0.15)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Dropout = 0.25"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, dropout=0.25)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Dropout = 0.3"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, dropout=0.3)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Dropout = 0.5"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Dropout = 0.7"""

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, dropout=0.7)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### Early Stopping

#### patience = 3
"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### patience = 5"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### patience = 8"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=8)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### patience = 10"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### Ρυθμός Μάθησης

#### learning_rate = $5 * 10^{-6}$
"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000005)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### learning_rate = $10^{-5}$"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.00001)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### learning_rate = $2.5 * 10^{-5}$"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### learning_rate = $5 * 10^{-5}$"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### learning_rate = $10^{-4}$"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.0001)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### learning_rate = $5 * 10^{-4}$"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.0005)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### learning_rate = $5 * 10^{-3}$"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.005)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### Μέγεθος Δέσμης

#### Batch Size = 32
"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Batch Size = 64"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Batch Size = 128"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Batch Size = 256"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### Optimizer

#### Adam
"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### Adamax"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025, optimizer="Adamax")
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""#### SGD"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, optimizer="SGD")
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""## Παρατηρήσεις ως προς τον αριθμό κλάσεων

### 20 Κλάσεις
"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 50, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### 40 Κλάσεις"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 70, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)

"""### 60 Κλάσεις"""

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)

DENSE201_MODEL = init_DENSE201_model(summary = True, inp_size=96, data_augment=True, learning_rate=0.000025)
print("Memory usage according to parameters:",get_model_memory_usage(BATCH_SIZE, DENSE201_MODEL)*1024,"MB")
print("")

start_time = time.time()
DENSE201_MODEL_history = train_model(DENSE201_MODEL, 100, 40, 10, 96,callback)
elapsed_time = time.time() - start_time

print(" ")
print("Time for training the model:",round(elapsed_time,2),"sec")
print(" ")

model_report(DENSE201_MODEL, DENSE201_MODEL_history, 30, 96)